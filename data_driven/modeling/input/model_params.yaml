model:
  DTC:
    model_params:
      default:
        ccp_alpha: 0.0
        class_weight: balanced
        criterion: gini
        max_depth: null
        max_features: null
        max_leaf_nodes: null
        min_impurity_decrease: 0.0
        min_impurity_split: null
        min_samples_leaf: 1
        min_samples_split: 2
        min_weight_fraction_leaf: 0.0
        random_state: 0
        splitter: best
      defined:
      for_tuning:
  RFC:
    model_params:
      default:
        bootstrap: True
        ccp_alpha: 0.0
        class_weight: balanced
        criterion: gini
        max_depth: null
        max_features: auto
        max_leaf_nodes: null
        max_samples: null
        min_impurity_decrease: 0.0
        min_impurity_split: null
        min_samples_leaf: 1
        min_samples_split: 2
        min_weight_fraction_leaf: 0.0
        n_estimators: 100
        n_jobs: 4
        oob_score: False
        random_state: 0
        verbose: 0
        warm_start: True
      defined:
      for_tuning:
  GBC:
    model_params:
      default:
        base_score: 0.5
        booster: gbtree
        colsample_bylevel: 1
        colsample_bynode: 1
        colsample_bytree: 1
        gamma: 0
        gpu_id: -1
        importance_type: gain
        interaction_constraints: ''
        learning_rate: 0.300000012
        max_delta_step: 0
        max_depth: 6
        min_child_weight: 1
        missing: .NAN
        monotone_constraints: !!python/tuple []
        n_estimators: 100
        n_jobs: 24
        num_parallel_tree: 1
        random_state: 0
        reg_alpha: 0
        reg_lambda: 1
        subsample: 1
        tree_method: exact
        validate_parameters: 1
        verbosity: None
        use_label_encoder: False
        objective: 'multi:softmax'
        eval_metric: mlogloss
      defined:
      for_tuning:
  ANNC:
    model_params:
      default:
        activation: relu
        alpha: 0.0001
        batch_size: auto
        beta_1: 0.9
        beta_2: 0.999
        early_stopping: False
        epsilon: 1e-08
        hidden_layer_sizes: !!python/tuple [100]
        learning_rate: constant
        learning_rate_init: 0.001
        max_fun: 15000
        max_iter: 200
        momentum: 0.9
        n_iter_no_change: 10
        nesterovs_momentum: True
        power_t: 0.5
        random_state: 0
        shuffle: True
        solver: adam
        tol: 0.0001
        validation_fraction: 0.1
        verbose: False
        warm_start: False
      defined:
      for_tuning: